---
title: "biostat"
author: "camille mathilde"
date: "22/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(dplyr)
library(tidyr)
library(kableExtra)
library(pander)
library(corrplot)
library(ggplot2)
```
\newpage

# Introduction 

De nos jours, les maladies sont de plus en plus étudiées et de mieux en mieux comprises. De nombreux organismes et intituts recherchent des solutions pour vaincre ces maladies en trouvant des traitements. De nombreuses diciplines sont impliquées, notamment la biostatistique, qui permet d'étudier différentes situations avec des données. Dans notre cas, nous allons étudier l'occurence des maladies coronarienne du coeur. Une étude préalable à déja été faite sur une cohorte d'individus. Ces individus ont répondu à une date donnée à une enquête sur les habitudes alimentaires. Les réponses ont été recueilli dans une base de donnée que l'on nomme \textbf{Coeur}. \
L'objectif de cette étude sera de connaître les bonnes habitudes alimentaires à adopter pour se prevenir d'une maladie coronarienne.

## Présentation de la base de donnée

Notre table de données contient 337 individus et 15 variables qui sont

\begin{itemize}
\item \textbf{Id} : l'identifiant du sujet. \
\item	\textbf{DateEntrée} et \textbf{Date de sortie} : les dates d’entrée et de sortie de l’étude. \
\item	\textbf{Date Naissance} : la date de naissance. \
\item	\textbf{Statut} : si la sortie de l’enquête est due à une maladie coronarienne de cœur, alors le type de maladie est indiqué (la signification du code n’est pas précisée ici). Si l’individu est sain à la sortie  de l’enquête, alors le code vaut 0.\
\item	\textbf{Emploi} : le type d’emploi.\
\item	\textbf{MoisEnquête} : le mois (1= Janvier, 12= Décembre) où l’individu a répondu à l’enquête sur ses pratiques alimentaires.\
\item	\textbf{Taille/Poids} : la taille et le poids de l'individu (en cm et en kg).\
\item	\textbf{Graisse} : la quantité moyenne de graisse ingérée par jour (g/jour).\
\item	\textbf{Fibres} : la quantité moyenne de fibres ingérée par jour (g/jour).\
\item	\textbf{Consommation} : la quantité de calories(/100) ingérée par jour.\
\item	\textbf{hauteConsomation} : une variable binaire, recodage de la variable consommation.\
\item	\textbf{MCC} : une variable binaire, recodage de la variable statut (1=MCC, 0=pas de MCC). \
\end{itemize}

Nous avons également rajouté la variable $\textbf{IMC}$ en divisant le poids par la taille au carrée pour faire un lien entre la condition physique et la maladie du coeur car le poids ou la taille tout seul ne suffisent pas pour savoir si une personne est en bonne santé ou en surpoids. \

Nous nous sommes également rendu compte qu'il y avait des erreurs dans le recodage de la variables \textbf{statut}. En effet, certains individus avaient contracté une maladie du coeur, mais la variable recodage \textbf{MCC} ne l'avait pas pris en compte nous avons  donc rectifié ça. Nous avons aussi remarqué que certaines variables qualitatives étaient en \textbf{numeric}, ce qui posera problème pour notre étude. Nous les recodons donc en \textbf{factor}.\
Nous observons enfin que la table de données contient des valeurs manquantes. Nous enlevons donc chaque ligne qui contient au moins une valeur manquante. Nous passons donc de 337 à 328 individus. \

Regardons les 5 premières lignes de notre table de données : \

```{r}
coeur <- readRDS('data/my_data_frame.rds')
coeur <-coeur%>%drop_na()
coeur <-coeur%>%dplyr::select(-X1)
coeur$statut<-as.factor(coeur$statut)
coeur$emploi<-as.factor(coeur$emploi)
coeur$moisEnqu_e<-as.factor(coeur$moisEnqu_e)
coeur$hauteConsomation<-as.factor(coeur$hauteConsomation)

coeur<-mutate(coeur,imc =poids /(taille/100)^2)
coeur<-coeur %>% dplyr::select(-MCC)
coeur <-mutate(coeur,MCC =case_when(
  statut!=0~1,
  TRUE~0))

pander(head(coeur))
```
\

Remarquons que cet ensemble d'individu est bien une cohorte car nous avons relevé certaines covariables et les trois données fondamentales qui sont, la date d'entrée dans l'étude, la date de sortie dans l'étude et le cause de sortie dans l'étude. Nous pouvons ajouter que les covariables utilisées dans l'étude sont fixe.


\newpage

# Statistique descriptive 

Notre jeu de donnée présente 5 variables quantitatives et 9 variables qualitatives.

Faisons un sommaire des variables quantitatives : 
```{r,echo=FALSE,fig.width=6, fig.height=4,fig.align='center'}
df_quanti<-coeur%>%dplyr::select(consommation,fibre,graisse,taille,poids)%>%summary()
pander(df_quanti)
```

Les individus mangent en moyenne 2835 calories par jours. Ils ingèrent de plus 12.76 grammes de gras par jour, mesurent 173 cm et pèsent 72.40 kilo-gramme en moyenne. \

Faisons maintenant un sommaire des variables quantitatives : 

```{r,echo=FALSE,fig.width=6, fig.height=4,fig.align='center'}
df_quali<-coeur%>%dplyr::select(statut,emploi,moisEnqu_e,hauteConsomation)%>%summary()
pander(df_quali)
```

Il y a 2 fois plus de $\textbf{Bank worker}$ que de $\textbf{Conductor}$ ou de $\textbf{Driver}$. Il y a $149$ personnes qui mangent moins de $2750$ calories par jours et $179$ personnes qui en mangent plus. \

Regardons désormais la corrélation entre les variables quantitatives : \

```{r,fig.width=4.1, fig.height=2.7,fig.align='center',echo=FALSE}
df_quanti1<-coeur%>%dplyr::select(consommation,fibre,graisse,taille,poids)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(df_quanti1), method="color", col=col(200), type="upper", order="hclust",
addCoef.col = "black", tl.col="black", tl.srt=45, diag=FALSE)

```

Nous observons que toutes les variables sont corrélées positivement. \ 


La variable \textbf{consommation} est très corrélée avec la variable \textbf{graisse} mais elle est très peu corrélée avec la variable \textbf{taille}. La variable \textbf{taille} est peu corrélée avec la variable \textbf{graisse}. \

Nous allons maintenant regarder le lien entre la maladie et le secteur d'activité. \

```{r,fig.width=5, fig.height=3.5,fig.align='center',echo=FALSE}
barplot(prop.table(table(coeur$emploi,coeur$MCC),margin=2),
beside=TRUE, ylim=c (0 ,1),legend.text = T,
main="Malade en fonction du secteur d'activité ",xlab ="MCC",ylab="Fréquence")
```

Nous observons que la proportion de conducteur est plus élevée chez les malades que chez les non malades.

Nous allons maintenant regarder le lien entre l'IMC et les maladies coronariennes.

```{r, fig.width=4.5, fig.height=3.5,fig.align='center',echo=FALSE}
coeur$MCC<-as.factor(coeur$MCC)
p<-ggplot(coeur, aes(x=MCC, y=imc, color=MCC)) +
  geom_boxplot()
p
```

Nous remarquons que la mediane de l'IMC du groupe de personnes malades est à peine plus basse que la médiane pour le groupe de personnes non malade. Une simple analyse descriptive ne suffit pas pour obtenir des résultats bien concluants, nous allons continuer avec des méthodes plus poussées. Nous commencerons par des régréssions logistiques.

# MCC expliqué par le poids et l'IMC

Nous allons commencer par une régréssion logistique qui nous permettera d'expliquer la variable $\textbf{MCC}$ en fonction de certaines covariables. Rappelons que dans la régréssion logistique ce n'est pas la réponse binaire qui est modélisé mais la probabilité de réalisation d'une des deux modalités (avoir une maladie coronarienne ou non). \

Nous allons commencer par regarder le meilleur modèle en comparant les AIC puis nous étudierons ce modèle. \

```{r,}
library(MASS)
coeurlogbin<-coeur%>%dplyr::select(-id,-dateEntree,-dateSortie,-dateNaissance,-statut)

res<-glm(MCC~.,family = binomial(logit),data=coeurlogbin)
stepAIC(res)
```

Le modèle ayant le plus faible AIC et que nous allons donc etudier est le suivant : 
$$y=\mu +  \beta_1 poids + \beta_2 imc$$

Regardons tout d'abord les relations entre les maladies coronariennes et l'imc. \

```{r,}
ggplot(coeurlogbin,aes(x=coeurlogbin$imc,y=coeurlogbin$MCC))+geom_point()

```
Nous osbervons qu

Regardons les relations entre les maladies coronariennes et le poids. \

```{r,}
ggplot(coeurlogbin,aes(x=coeurlogbin$poids,y=coeurlogbin$MCC))+geom_point()

```

Faisons une régression logisitique : \

```{r,}
reslog<-glm(MCC~imc+poids,family = binomial(logit),data=coeurlogbin)
summary(reslog)

```

Nous vérifions tout d'abord les conditions d'application de la régréssion logistique. Il est recommandé d'avoir en pratique 10 fois plus d'évenements que de paramètres dans le modèle. Nous utilisons ici 3 paramétres nous devrions donc avoir au moins 30 malades. 

```{r,}
table(coeurlogbin$MCC)
```
Nous avons 76 malades nous pouvons donc continuer. \

Il faut maintenant vérifier que nous ne sommes pas dans le cas de surdipersion c'est à dire qu'il ne faut pas que la dispersion réelle des données soit supérieure à celle prévue par la théorie car dans ce cas l'erreur standard des paramètres est sous-estimée ce qui peut conduire à des p-valeurs très faible et donner des conclusions erronnées. Evaluons donc s'il y a ou non une surdispersion : $$\frac{deviance residuelle}{nddl}=\frac{346.17}{325}=1.06$$ nous pouvons ainsi considérer qu'il n'y a pas surdispersion. \


Nous pouvons maintenant passer à l'interprétation des résultats de la régréssion logistique. \

```{r,}
reslog<-glm(MCC~imc+poids,family = binomial(logit),data=coeurlogbin)
summary(reslog)

```
Notons tout d'abord que les p-values de imc et poids sont inférieurs au seuil de $5\%$, les effets des variables explicatives sont donc significatifs au seuil de $5 \%$. L'intercept n'est pas significatif nous le retirons donc du modèle. \

```{r,}
reslog<-glm(MCC~imc+poids-1,family = binomial(logit),data=coeurlogbin)
reslog
summary(reslog)

```

Nous observons que les p-valeurs sont toutes inférieures au seuil de $5 \%$. De plus $\hat \beta_1=0.17541$ et $\hat \beta_2=-0.07536$. \

Nous ajoutons de plus les intervalles de confiances qui sont primordiaux pour effectuer une analyse et une interprétation. Nous pouvons faire des intervalles de confiance car nous disposons d'un assez grand jeu de donnés contenant 328 observations. \

```{r,}
confint(reslog)
```
Notons que les intervalles de confiance sont interprétable car ils ne contiennt pas 0.\


Nous observons au premier abord (sans regarder les intervalles de confiance) que plus l'imc est élevé plus le risque d'être malade augmente car le signe du coefficient (0.17541) est positif. Au contraire plus le poids est élevé, plus le risque d'être malade diminue car le signe du coefficient (-0.07536) est négatif. Ainsi nous pouvons dire plus simplement que si l'imc augmente, le risque d'être malade augmente. \
Adaptons une analyse plus précise en quantifiant : 

Nous augmentons l'imc  de 1 $(x_1 + 1)$ et nous laissons le poids fixé à $x_2$. \
Si l'imc augemente de 1, l'odds d'être malade est $exp(\beta_{1}) exp(x_1 \beta_{1} + x_2 \beta_{2} )$

Nous pouvons donc voir qu'a poids fixé, augmenter l'imc de 1 va multiplier l'odds d'avoir une MCC par au moins $\exp(0.02898397)=1.03$  et au plus $\exp(0.32575504)=1.39$. Nous pouvons de plus voir qu'a imc fixé, augmenter le poids de 1 va multiplier l'odds d'avoir une MCC par au moins $\exp(-0.12623701)=0.88$ et au plus $\exp(-0.02628422)=0.97$.  \

Fixer une imc et augmenter le poids signifie que nous considèrons que la personne prend en taille ce qui paraît un peu hors réalite. En effet, si nous prenons une personne de 60 kg pour 1m65, l'imc est de 22.04. Si nous fixons l'imc à 22.04 et que cette personne prend 1kg, pour ne pas changer d'imc elle devra avoir pris également 1.4 cm en plus. \

En conclusion de cette partie, il faut retenir que plus l'imc d'une personne augmente, plus le risque q'elle tombe malade d'une maladie coronarienne diminue. Avoir un imc stable et assez bas est un facteur protecteur du risque de maladie coronarienne.


# MCC expliqué par la consommation de fibre

Nous trouvons également intéressant de regarder le lien entre la consommation de fibre et les maladies coronariennes. \

Regardons la relation entre les maladies coronariennes et la consommation de fibre. \

```{r,}
ggplot(coeurlogbin,aes(x=coeurlogbin$fibre,y=coeurlogbin$MCC))+geom_point()

```

Nous observons que les personnes mangeant plus de 2g de fibre par jour sont moins malades. \

Effectuer désormais la régréssion logistique. \
```{r,}
reslog<-glm(MCC~fibre-1,family = binomial(logit),data=coeurlogbin) #nous enelevons l'intercept qui n'est pas significatif
summary(reslog)
confint(reslog)
```

Augmenter la consommation de fibre de 1g par jour va multiplier l'odds d'avoir une MCC par au moins $\exp(-0.8604690)=0.43$  et au plus $\exp(-0.5542772)=0.57$. Autrement dit, si nous augmentons notre consommation de fibre de 1g par jour nous divisons par au moins (1/0.43)=2.32 l'odds de contracter une maladie coronarienne. \



# régréssion polytomique ordonné

```{r,}
coeur <-mutate(coeur,consom_dec =case_when(
  consommation<23~"peu",
  consommation>22 & consommation<30~"moyen",
  consommation>29 ~"beaucoup"))
coeur$consom_dec<-as.factor(coeur$consom_dec)
```

```{r,}
coeurlogbin1<-coeur%>%dplyr::select(-id,-dateEntree,-dateSortie,-dateNaissance,-statut,-hauteConsomation,-consommation)
modele<-polr(consom_dec~.,data=coeurlogbin1)
stepAIC(modele)
```








```{r,}
modele2<-polr(consom_dec~graisse+fibre,data=coeurlogbin1)
modele2
confint(modele2)
```

La commande polyr utilisée renvoie l'opposé du coefficient $\beta$ considéré donc nous obtenons les résultats suivants pour nos coefficients: \

QUESTION SUR LES MOINS

$$ln \bigg(\frac{odds(Y \leq moyen | graisse = x_1+1, fibre=x_2)}{odds(Y \leq moyen | graisse = x_1, fibre=x_2)}\bigg)=\beta_{graisse}=1.29$$

A consommation de fibre fixée, augmenter la consommation de graisse de 1 g/jour va diviser $odds(Y \leq moyen)$ par au moins $exp(-1.56)$ et au plus $exp(-1.05)$. \ QUEST

$$ln \bigg(\frac{odds(Y \leq moyen | fibre = x_1+1, graisse=x_2)}{odds(Y \leq moyen | fibre = x_1, graisse=x_2)}\bigg)=\beta_{fibre}=1.81$$

A consommation de graisse fixée, augmenter la consommation de fibre de 1 g/jour va diviser $odds(Y \leq moyen)$ par au moins $exp(-2.599)$ et au plus $exp(-1.07)$. \ QUEST

# régréssion polytomique non-ordonné


```{r,results=FALSE}
library(nnet)
modele3<-multinom(emploi~.,data=coeurlogbin1)
stepAIC(modele3)
```

```{r,}
modele4<-multinom(emploi~poids+taille,data=coeurlogbin1)
modele4
confint(modele4)
```

Bank worker est la modalité de référence pour emploi. \

$$\beta_{poids|conductor}=ln \bigg(\frac{\frac{P(conductor| poids = x_1+1, taille=x_2)}{P(Bank worker | poids = x_1+1, taille=x_2)}}{\frac{P(conductor | poids = x_1, taille=x_2)}{P(Bank worker | poids = x_1,taille=x_2)}}\bigg)=-0.07620753 $$  

QUESTION POUR LES MO DE REF X1 X2

Toutes les autres covariables fixées à leurs modalités de référencee, un 1kg en plus va multiplier par au moins $exp(-0.1114239)=$ la préférence de conductor par rapport à Bank worker. \



$$\beta_{taille|conductor}=ln \bigg(\frac{\frac{P(conductor| taille = x_1+1, poids=x_2)}{P(Bank worker | taille = x_1+1, poids=x_2)}}{\frac{P(conductor | taille = x_1, poids=x_2)}{P(Bank worker | taille = x_1,poids=x_2)}}\bigg)=-0.1694273$$  

QUESTION POUR LES MO DE REF X1 X2

Toutes les autres covariables fixées à leurs modalités de référencee, un 1cm en plus va multiplier par au moins $exp( -0.1959546)=$ la préférence de conductor par rapport à Bank worker. \


# Modèle de Cox

Dans cette partie, nous allons chercher à répondre à la problématique suivante :  à une date donnée, quelle sera le taux de nouveaux malades dans la population étudiée ?


<!-- Les paramètres à estimer sont fonction d'incidence instentatnée, sera les lambda, represente le fait que à une date donnée quelle estt le taux de production de nouveau malade qui va avoir lieux,Ce n'est pas les lambda que lon estime, on estime les parametres d'un modeles de cox qui sont des effets multiplcatifs entre différents lambda, pour chaque sous population possible, on a une fonction laambda et cette fonction lambda nous dit lamambda de t, c est a une date t combien , quelle est le taux de production de nouveau malade. -->
<!-- Dans notre siuation, il y a aucune information sur la date d'entrée , on les ait mis dans un certain etat particulier  -->

Nous allons désormais nettoyer la base de données et faire des transformations de format des variables pour pouvoir utiliser le modèle de Cox.
Les variables qui contiennent une date ( date entrée, date sortie et date de naissance) sont de type "charactère" nous allons donc dans un premier temps les convertir en type "Date".

```{r,}
library(lubridate)
coeur$dateEntree<-as.Date(coeur$dateEntree,format="%d/%m/%y")
year(coeur$dateEntree)<-1900+year(coeur$dateEntree) %% 100
coeur$dateSortie<-as.Date(coeur$dateSortie,format="%d/%m/%y")
year(coeur$dateSortie)<-1900+year(coeur$dateSortie) %% 100

coeur<-coeur%>% mutate(time = coeur$dateSortie - coeur$dateEntree )
```

Ensuite, la fonction qui exécute le modèle de cox à besoin de valeurs numériques représentant les dates. Sous R, chaque date est repésentée par un nombre de jour à partir d'une date d'origine : le 1 janvier 1970. Nous allons donc créer une variable qui récupérera ce nombre pour chaque date correspondant à chaques individus.

```{r}
coeur2<-coeur
year(coeur2$dateEntree)<-1900+year(coeur2$dateEntree) %% 100
coeur2<-coeur2%>%mutate(date_entree_num=as.numeric(as.Date(coeur2$dateEntree)))
year(coeur2$dateSortie)<-1900+year(coeur2$dateSortie) %% 100
coeur2<-coeur2%>%mutate(date_sortie_num=as.numeric(as.Date(coeur2$dateSortie)))
```

Nous pouvons désormais faire nos analyse avec le modèle de Cox. 
Pour une première analyse, nous estimerons notre modèle en prenant en compte toute les covariables possible présentent dans la table de données. Nous obtenons les résultats suivant :

```{r}
coeurcox<-coeur%>%dplyr::select(-id,-dateEntree,-dateSortie,-dateNaissance,-statut,-hauteConsomation,-moisEnqu_e)
 library(survival)
  survie=Surv(coeur2$date_entree_num,coeur2$date_sortie_num,coeur$MCC)
 res=coxph(survie~coeur2$fibre+coeur2$taille+coeur2$poids+coeur2$consommation+coeur2$emploi+coeur2$graisse,id=coeur2$id)
```

Avant d’interpréter plus le modèle, il est utile de voir si l’hypothèse d'indépendance du temps des formes multiplicatives et les covariables est vérifiée.
Cette vérification est à faire sur le modèle global, avant même d’interpréter les tests (qui ne sont pas valables lorsque l’hypothèse n’est pas vérifiée), et avant de sélectionner des variables : il se peut qu’une covariable ait un effet non significatif lorsque cet effet est moyenné dans le temps mais qu’elle ait une interaction significative avec le temps. C’est pourquoi il vaut mieux tester l’hypothèse de HP avant d’interpréter la significativité des effets.

```{r}
res.c=cox.zph(res)
res.c
```

Le test global de validité de l’hypothèse de HP conduit à ne pas rejeter cette hypothèse au seuil de $5\%$ : aucune covariable n'a un effet dépendant du temps. 

Il est possible de générer des graphiques de résidus pour des prédicteurs individuels. Dans les graphiques, une pente non nulle est une preuve contre la proportionnalité. 
Remarque : R trace le graphe des résidus en incluant par défaut un lissage par des splines (trait plein), et des intervalles de confiance à 95%.

```{r}
par(mfrow=c(2,4))
plot(res.c)
```
\
A l'oeil nu, nous observons qu'à peu près toutes les droites sont à l'horizontales. La droite dans le graphique de emploie est un peu inclinée mais comme la $p_{valeur}$ est supérieure à 5 $\%$, nous allons considérer que cette covariable reste indépendante du temps. En revanche la droite de consommation est aussi un peu plus inclinée mais sa $p_{valeur}= 0.085$ est supérieur à 5 $\%$ mais pas supérieur à 10 $\%$. Nous allons donc considérer que cette covariable dépend du temps. Il ne serait pas judicieux de l'étudier avec le modèle de cox à risques proportionnelles.

```{r}
 res=coxph(survie~coeur2$fibre+coeur2$taille+coeur2$poids+coeur2$emploi+coeur2$graisse,id=coeur$id)
 summary(res)
```


Le test de Wald teste l’effet d’une covariable, les autres étant dans le modèle. S’il n’est pas significatif, cela ne veut pas dire qu’il ne le serait pas dans le modèle constitué uniquement de cette covariable.

Le test de Wald pour la covariable « taille » montre que le coefficient correspondant est fortement significatifs au seuil $5\%$ $p_{values} < 0.05$ : cette covariable a un effet important sur l'incidence instentannée à un moment t. Les autres covariables ne modifient pas significativement cette incidence lorsque taille est dans le modèle.

Nous sélectionnons les variables pas à pas c'est à dire que nous enlèvons celle dont la statistique de Wald est la plus faible avec $p_{valeur}$ la plus élevée. Ensuite nous refaisons tourner le modèle et nous recommençons jusqu’à obtention de toutes les variables significatives.  

Nous obtenons le modèle final suivant :

```{r}
res=coxph(survie~coeur2$taille,id=coeur$id)
summary(res)
confint(res)
```

Les p-values des 3 tests sont très inférieures à 5%, ce qui montre que l’ajustement d’un modèle de Cox est très pertinent au seuil
5%.

Seul la co-variable « taille » a un coefficients fortement significatifs au seuil 5% $p_{values} < 0.05$. 

L’exponentielle des coefficients mesure l’effet multiplicatif d’une augmentation de la covariable d’une unité sur le taux,
toutes choses égales par ailleurs.

En supposant que toutes les autres covariables restent constantes, la taille a un effet positif sur le taux de nouveaux malades à un instant donnée, l’effet d’une augmentation de taille de 1 unité réduit le taux de nouveau malade à l'instant t par un facteur de en moyenne,  au moins, $exp(-0.08944234)=0.9144$
c’est à dire par $100\%-91,44\%=8,56\%$.


Auparavant, nous avont constater que dans le modèle global, tout les autres coefficients n'étaient pas significatifs au seuil de 5 pourcent. Mais qu'en est t-il si on test avec un modèle avec seulement 1 covariable. Regardons ce qui se passe pour fibre :
```{r}
res=coxph(survie~coeur$fibre,id=coeur$id)
summary(res)

confint(res)
```
« fibre » a un coefficients significatifs au seuil 5% $p_{values} < 0.05$. 

En supposant que toutes les autres covariables restent
constantes, fibre a un effet positif sur le taux de nouveaux malades à un instant donnée, l’effet d’une augmentation de taille de 1 unité réduit le taux de nouveau malade à l'instant t par un facteur de,  au moins, $exp(-0.5315  )=0.5877227$
c’est à dire par $100\%-58.77\%=41,33\%$.

Testons maintenant pour la variable graisse
```{r}
 res=coxph(survie~coeur$graisse,id=coeur$id)
summary(res)
confint(res)
```
« graisse » a une $p_{values} > 0.05$. De plus, l'intervalle de confiance n'est pas interpretable car il contient 0. Nous ne pouvons pas faire d'estimation avec ce modèle.


```{r}
res=coxph(survie~coeur2$emploi,id=coeur2$id)
summary(res)
confint(res)
```
L'intervalle de confiance comprend 0, rien n'est interpretable


# Conclusion : Comment diminuer les rirsques d'avoir une maladie coronarienne.

Manger des fibres
