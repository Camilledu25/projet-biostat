---
title: "biostat"
author: "camille mathilde"
date: "22/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(dplyr)
library(tidyr)
library(kableExtra)
library("survminer")

library(pander)
library(corrplot)
library(ggplot2)
library(MASS)
```
\newpage

# Introduction 

De nos jours, les maladies sont de plus en plus étudiées et de mieux en mieux comprises. De nombreux organismes et instituts recherchent des solutions pour vaincre ces maladies en trouvant des traitements. De nombreuses diciplines sont impliquées, notamment la biostatistique, qui permet d'étudier différentes situations avec des données. Dans notre cas, nous allons étudier l'occurence des maladies coronarienne du coeur. Une étude préalable à déja été faite sur une cohorte d'individus. Ces individus ont répondu à une date donnée à une enquête sur les habitudes alimentaires. Les réponses ont été recueilli dans une base de donnée que l'on nomme \textbf{Coeur}. \
L'objectif principal de cette étude sera de connaître les bonnes habitudes alimentaires à adopter pour se prevenir d'une maladie coronarienne.
\newpage
\tableofcontents
\newpage

## Présentation de la base de donnée

Notre table de données contient 337 individus et 15 variables qui sont

\begin{itemize}
\item \textbf{Id} : l'identifiant du sujet. \
\item	\textbf{DateEntrée} et \textbf{Date de sortie} : les dates d’entrée et de sortie de l’étude. \
\item	\textbf{Date Naissance} : la date de naissance. \
\item	\textbf{Statut} : si la sortie de l’enquête est due à une maladie coronarienne de cœur, alors le type de maladie est indiqué (la signification du code n’est pas précisée ici). Si l’individu est sain à la sortie  de l’enquête, alors le code vaut 0.\
\item	\textbf{Emploi} : le type d’emploi.\
\item	\textbf{MoisEnquête} : le mois (1= Janvier, 12= Décembre) où l’individu a répondu à l’enquête sur ses pratiques alimentaires.\
\item	\textbf{Taille/Poids} : la taille et le poids de l'individu (en cm et en kg).\
\item	\textbf{Graisse} : la quantité moyenne de graisse ingérée par jour (g/jour).\
\item	\textbf{Fibres} : la quantité moyenne de fibres ingérée par jour (g/jour).\
\item	\textbf{Consommation} : la quantité de calories(/100) ingérée par jour.\
\item	\textbf{hauteConsomation} : une variable binaire, recodage de la variable consommation.\
\item	\textbf{MCC} : une variable binaire, recodage de la variable statut (1=MCC, 0=pas de MCC). \
\end{itemize}

Nous avons également rajouté la variable $\textbf{IMC}$ en divisant le poids par la taille au carrée pour faire un lien entre la condition physique et la maladie du coeur car le poids ou la taille tout seul ne suffisent pas pour savoir si une personne est en bonne santé ou en surpoids. \

Nous nous sommes également rendu compte qu'il y avait des erreurs dans le recodage de la variables \textbf{statut}. En effet, certains individus avaient contracté une maladie du coeur, mais la variable recodage \textbf{MCC} ne l'avait pas pris en compte nous avons  donc rectifié ça. Nous avons aussi remarqué que certaines variables qualitatives étaient en \textbf{numeric}, ce qui posera problème pour notre étude. Nous les recodons donc en \textbf{factor}.\
Nous observons enfin que la table de données contient des valeurs manquantes. Nous enlevons donc chaque ligne qui contient au moins une valeur manquante. Nous passons donc de 337 à 328 individus. \

Regardons les 5 premières lignes de notre table de données : \

```{r}
coeur <- readRDS('data/my_data_frame.rds')
coeur <-coeur%>%drop_na()
coeur <-coeur%>%dplyr::select(-X1)
coeur$statut<-as.factor(coeur$statut)
coeur$emploi<-as.factor(coeur$emploi)
coeur$moisEnqu_e<-as.factor(coeur$moisEnqu_e)
coeur$hauteConsomation<-as.factor(coeur$hauteConsomation)

coeur<-mutate(coeur,imc =poids /(taille/100)^2)
coeur<-coeur %>% dplyr::select(-MCC)
coeur <-mutate(coeur,MCC =case_when(
  statut!=0~1,
  TRUE~0))

pander(head(coeur))
```
\

Remarquons que cet ensemble d'individu est bien une cohorte car nous avons relevé certaines covariables et les trois données fondamentales qui sont, la date d'entrée dans l'étude, la date de sortie dans l'étude et le cause de sortie dans l'étude. Nous pouvons ajouter que les covariables utilisées dans l'étude sont fixe.


\newpage

# Statistique descriptive 

Notre jeu de donnée présente 5 variables quantitatives et 9 variables qualitatives.

Faisons un sommaire des variables quantitatives : 
```{r,echo=FALSE,fig.width=6, fig.height=4,fig.align='center'}
df_quanti<-coeur%>%dplyr::select(consommation,fibre,graisse,taille,poids)%>%summary()
pander(df_quanti)
```

Les individus mangent en moyenne 2835 calories par jours. Ils ingèrent de plus 12.76 grammes de gras et 1.723 grammes de fibre par jour, mesurent 173,4 cm et pèsent 72.40 kilo-gramme en moyenne. \

Faisons maintenant un sommaire des variables qualitatives : 

```{r,echo=FALSE,fig.width=6, fig.height=4,fig.align='center'}
df_quali<-coeur%>%dplyr::select(statut,emploi,moisEnqu_e,hauteConsomation)%>%summary()
pander(df_quali)
```

Il y a 2 fois plus de $\textbf{Bank worker}$ que de $\textbf{Conductor}$ ou de $\textbf{Driver}$. Il y a $149$ personnes qui mangent moins de $2750$ calories par jours et $179$ personnes qui en mangent plus. \

Regardons désormais la corrélation entre les variables quantitatives : \

```{r,fig.width=4.1, fig.height=2.7,fig.align='center',echo=FALSE}
df_quanti1<-coeur%>%dplyr::select(consommation,fibre,graisse,taille,poids)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(df_quanti1), method="color", col=col(200), type="upper", order="hclust",
addCoef.col = "black", tl.col="black", tl.srt=45, diag=FALSE)

```

Nous observons que toutes les variables sont corrélées positivement. \ 


La variable \textbf{consommation} est très corrélée avec la variable \textbf{graisse} mais elle est très peu corrélée avec la variable \textbf{taille}. La variable \textbf{taille} est peu corrélée avec la variable \textbf{graisse}. \

Nous allons maintenant regarder le lien entre la maladie et le secteur d'activité. \

```{r,fig.width=5, fig.height=3.5,fig.align='center',echo=FALSE}
barplot(prop.table(table(coeur$emploi,coeur$MCC),margin=2),
beside=TRUE, ylim=c (0 ,1),legend.text = T,
main="Malade en fonction du secteur d'activité ",xlab ="MCC",ylab="Fréquence")
```

Nous observons que la proportion de conducteur est plus élevée chez les malades que chez les non malades.

Nous allons maintenant regarder le lien entre l'IMC et les maladies coronariennes.

```{r, fig.width=4.5, fig.height=3.5,fig.align='center',echo=FALSE}
coeur$MCC<-as.factor(coeur$MCC)
p<-ggplot(coeur, aes(x=MCC, y=imc, color=MCC)) +
  geom_boxplot()
p
```

Nous remarquons que la mediane de l'IMC du groupe de personnes malades est à peine plus basse que la médiane pour le groupe de personnes non malade. Une simple analyse descriptive ne suffit pas pour obtenir des résultats bien concluants, nous allons continuer avec des méthodes plus poussées. Nous commencerons par des régréssions logistiques.



# Etude de la consommation de fibre

## Régression logistique binaire

Nous allons commencer par une régréssion logistique qui nous permettera d'expliquer la variable $\textbf{MCC}$ en fonction de certaines covariables. Rappelons que dans la régréssion logistique ce n'est pas la réponse binaire qui est modélisée mais la probabilité de réalisation d'une des deux modalités (avoir une maladie coronarienne ou non). \

Nous allons commencer par regarder le meilleur modèle. Nous enlevons la $\textbf{taille}$ et le $\textbf{poids}$ pour laisser $\textbf{imc}$ car elles sont très fortement corrélées. Nous avons tout d'abord voulu comparer en comparant les AIC avec stepAIC. 

```{r,results=FALSE}

coeurlogbin<-coeur%>%dplyr::select(-id,-dateEntree,-dateSortie,-dateNaissance,-statut,
                                   -poids,-taille)
res<-glm(MCC~.,family = binomial(logit),data=coeurlogbin)
stepAIC(res)
```
Le modèle ayant le plus faible AIC est le suivant : 
$$y=\mu +  \beta_1 fibre$$  \

Nous avons également utilisé les p-values pour trouver le meilleur modèle en enlevant à la main petit à petit les variables les moins significatives et nous obtenons encore le même modèle.

```{r,results=FALSE}

coeurlogbin<-coeur%>%dplyr::select(-id,-dateEntree,-dateSortie,-dateNaissance,-statut,
                                   -poids,-taille)
res<-glm(MCC~.-emploi-moisEnqu_e-consommation-1-hauteConsomation-imc-graisse,
         family = binomial(logit),data=coeurlogbin)
summary(res)
```


Regardons donc la relation entre les maladies coronariennes et la consommation de fibre. \

```{r,}
ggplot(coeurlogbin,aes(x=coeurlogbin$fibre,y=coeurlogbin$MCC))+geom_point()

```

Nous observons que les personnes mangeant plus de 2g de fibre par jour sont moins malades. \

Nous voulons faire une régression logistique, nous allons donc vérifier les conditions d'application. Il est recommandé d'avoir en pratique 10 fois plus d'évenements que de paramètres dans le modèle. Nous allons utiliser ici 2 paramétres ( en comptant l'intercept) nous devrions donc avoir au moins 20 malades. 

```{r,}
table(coeurlogbin$MCC)
```
Nous avons 76 malades nous pouvons donc continuer. \

Il faut maintenant vérifier que nous ne sommes pas dans le cas de surdipersion c'est à dire qu'il ne faut pas que la dispersion réelle des données soit supérieure à celle prévue par la théorie car dans ce cas l'erreur standard des paramètres est sous-estimée ce qui peut conduire à des p-valeurs très faible et donner des conclusions erronées. Evaluons donc s'il y a ou non une surdispersion :

```{r,}
reslog<-glm(MCC~fibre,family = binomial(logit),data=coeurlogbin)
summary(reslog)
```

Nous avons ainsi $$\frac{deviance residuelle}{nddl}=\frac{351.12}{326}=1.08$$ nous pouvons donc considérer qu'il n'y a pas surdispersion. \

Nous pouvons donc maintenant utiliser la régression logistique et faire des interprétations : \

```{r,warning=FALSE,message=FALSE}
reslog<-glm(MCC~fibre-1,family = binomial(logit),data=coeurlogbin)
reslog
confint(reslog)
```

$$ln \bigg(\frac{odds(Y =1  | fibre = x_1+1)}{odds(Y=1 | fibre = x_1)}\bigg)=\hat{\beta}_{fibre}=-0.70288$$
Augmenter la consommation de fibre de 1g par jour va multiplier la chance d'avoir une maladie coronarienne par au moins $\exp(-0.8604690)=0.43$  et au plus $\exp(-0.5542772)=0.57$. Autrement dit, si nous augmentons notre consommation de fibre de 1g par jour nous divisons par au moins (1/0.43)=2.32 la chance de contracter une maladie coronarienne. \
Nous pouvons alors conseiller de manger des fibres si nous voulons nous prévenir des maladies coronariennes. 


# Etude de la consommation
## Régression polytomique ordonné

Nous voulons maintenant étudier la consommation à l'aide d'une régression polytomique ordonnée. En effet nous aimerions voir ce qui influence la consommation de calories des personnes. Nous voulons au moins 3 modalités, nous recodons donc la variable $\textbf{consommation}$ pour qu'elle soit qualitative à 3 modalités. Nous classons ainsi la consommation en "faible", "moyen" et "élevée". Nous avons alors bien une relation d'ordre $"faible" < "moyen" < "élevée"$ .\

```{r,}
coeur <-mutate(coeur,consom_dec =case_when(
  consommation<23~"faible",
  consommation>22 & consommation<30~"moyen",
  consommation>29 ~"élevée"))
coeur$consom_dec<-as.factor(coeur$consom_dec)
```


Nous faisons un stepAIC pour choisir le meilleur modèle au sens du critère $\textbf{AIC}$. \

```{r,results=FALSE}
coeurlogbin1<-coeur%>%dplyr::select(-id,-dateEntree,-dateSortie,-dateNaissance,-statut,
                                    -hauteConsomation,-consommation)
modele<-polr(consom_dec~.,data=coeurlogbin1)
stepAIC(modele)
```

Nous obtenons qu'il faut garder $\textbf{graisse}$ et $\textbf{fibre}$, nous faisons donc la régréssion polytomique ordonnée selon ces variables. \

```{r,warning=FALSE,message=FALSE}
modele2<-polr(consom_dec~graisse+fibre,data=coeurlogbin1)
modele2
confint(modele2)
```

Notons que la commande $polr$ utilisée renvoie l'opposé du coefficient $\beta$ considéré donc nous obtenons les "vrais" résultats suivants : $\hat{\beta}_{graisse}=0.43$ et $\hat{\beta_{fibre}}=0.59$ avec les intervalles de confiance suivant : $\hat{\beta}_{graisse}  \in [0.31,0.56]$ et  $\hat{\beta}_{fibre}  \in [0.12,1.10]$

Nous pouvons regarder noter que : 
$$ln \bigg(\frac{odds(Y \leq moyen | graisse = x_1+1, fibre=x_2)}{odds(Y \leq moyen | graisse = x_1, fibre=x_2)}\bigg)=\hat{\beta}_{graisse}$$

A consommation de fibre fixée, augmenter la consommation de graisse de 1 g/jour va multiplier l'odds de $Y \leq moyen$ par au moins  $exp(0.31)=1.36$ et au plus $exp(0.56)=1.75$. \ 

De plus : \
$$ln \bigg(\frac{odds(Y \leq moyen | fibre = x_1+1, graisse=x_2)}{odds(Y \leq moyen | fibre = x_1, graisse=x_2)}\bigg)=\hat{\beta}_{fibre}$$

A consommation de graisse fixée, augmenter la consommation de fibre de 1 g/jour va multiplier l'odds de $Y \leq moyen$ par au moins $exp(0.12)=1.12$ et au plus $exp(1.10)=3$. Ainsi manger plus de fibre va augmenter les chances que la personne mange moins de calorie dans la journée. Ceci n'est pas surpenant car les fibres sont connus pour être rassasiant. \

Nous avons vu précedemment qu'augmenter sa consommation de fibre était une bonne chose pour se prévenir des maladies coronariennes, nous savons maintenant que à consommation de graisse fixée la consommation de fibre permet également au personne de ne pas manger trop de calories. \

# Étude de l'emploi
## Régression polytomique non ordonné

Nous voulons maintenant étudier l'emploi à l’aide d’une régression polytomique non ordonnée. En effet nous aimerions voir ce qui influence la préférence de choisir un type d'emploi par rapport aux autres. La variable $\textbf{emploi}$ est composée de 3 modalités, qui sont : $conductor$, $bank worker$ et $driver$.  

Nous faisons un stepAIC pour choisir le meilleur modèle au sens du critère AIC : 

```{r,results=FALSE,include=FALSE}
library(nnet)
modele3<-multinom(emploi~.,data=coeurlogbin1)
stepAIC(modele3)
```

Nous obtenons qu’il faut garder les variables $\textbf{taille}$ et $\textbf{poids}$.
Nous faisons donc la régréssion polytomique non ordonnée selon ces variables.

```{r,message=FALSE,warning=FALSE}
modele4<-multinom(emploi~poids+taille,data=coeurlogbin1)
modele4
confint(modele4)
```

$\textbf{Bank worker}$ est la modalité de référence pour emploi. \

$$\hat{\beta}_{poids|conductor}=ln \bigg(\frac{\frac{P(conductor| poids = x_1+1, taille=x_2)}{P(Bank worker | poids = x_1+1, taille=x_2)}}{\frac{P(conductor | poids = x_1, taille=x_2)}{P(Bank worker | poids = x_1,taille=x_2)}}\bigg)=-0.07620753 $$  

À taille fixée, un poids de 1kg en plus va multiplier par au moins $exp(-0.1114239)=0.8945595$ et au plus $exp( -0.04099117)=0.9598376$ la préférence de conductor par rapport à Bank worker. \
En d'autres termes, à taille fixée une augmentation de 1kg va diviser la préférence de conductor par rapport à Bank Worker de au moins $(1/0.8945595)=1.12$. 

$$\hat{\beta}_{taille|conductor}=ln \bigg(\frac{\frac{P(conductor| taille = x_1+1, poids=x_2)}{P(Bank worker | taille = x_1+1, poids=x_2)}}{\frac{P(conductor | taille = x_1, poids=x_2)}{P(Bank worker | taille = x_1,poids=x_2)}}\bigg)=-0.1694273$$  

À poids fixé, une taille qui augmente de 1cm va multiplier par au moins $exp(-0.1959546)=0.82$ et au plus $exp( -0.14290007)=0.8668407$ la préférence de conductor par rapport à Bank worker. \


$$\hat{\beta}_{taille|driver}=ln \bigg(\frac{\frac{P(driver| taille = x_1+1, poids=x_2)}{P(Bank worker | taille = x_1+1, poids=x_2)}}{\frac{P(driver | taille = x_1, poids=x_2)}{P(Bank worker | taille = x_1,poids=x_2)}}\bigg)=-0.1231291$$

À poids fixé, une taille qui augmente de 1cm va multiplier par au moins $exp(-0.15673368)=0.8549317$ et au plus $exp( -0.08952445)=0.9143659$ la préférence de driver par rapport à Bank worker. \

Ainsi, la taille et le poids ont un effet sur le métier choisit.
\newpage

# Étude de l'incidence instantanée de la maladie


Dans cette partie, nous allons chercher à répondre à la problématique suivante :  à une date donnée, quel sera le taux de nouveaux malades dans la population étudiée ?


<!-- Les paramètres à estimer sont fonction d'incidence instentatnée, sera les lambda, represente le fait que à une date donnée quelle estt le taux de production de nouveau malade qui va avoir lieux,Ce n'est pas les lambda que lon estime, on estime les parametres d'un modeles de cox qui sont des effets multiplcatifs entre différents lambda, pour chaque sous population possible, on a une fonction laambda et cette fonction lambda nous dit lamambda de t, c est a une date t combien , quelle est le taux de production de nouveau malade. -->
<!-- Dans notre siuation, il y a aucune information sur la date d'entrée , on les ait mis dans un certain etat particulier  -->

Nous allons désormais nettoyer la base de données et faire des transformations de format des variables pour pouvoir utiliser le modèle de Cox.
Les variables qui contiennent une date ( date entrée, date sortie et date de naissance) sont de type "charactère" nous allons donc dans un premier temps les convertir en type "Date".

```{r,include=FALSE}
library(lubridate)
coeur$dateEntree<-as.Date(coeur$dateEntree,format="%d/%m/%y")
year(coeur$dateEntree)<-1900+year(coeur$dateEntree) %% 100
coeur$dateSortie<-as.Date(coeur$dateSortie,format="%d/%m/%y")
year(coeur$dateSortie)<-1900+year(coeur$dateSortie) %% 100

coeur<-coeur%>% mutate(time = coeur$dateSortie - coeur$dateEntree )
```

Ensuite, la fonction qui exécute le modèle de cox à besoin de valeurs numériques représentant les dates. Sous R, chaque date est repésentée par un nombre de jour à partir d'une date d'origine : le 1 janvier 1970. Nous allons donc créer une variable qui récupérera ce nombre pour chaque date correspondant à chaques individus.

```{r,include=FALSE}
 library(survival)
coeur2<-coeur
year(coeur2$dateEntree)<-1900+year(coeur2$dateEntree) %% 100
coeur2<-coeur2%>%mutate(date_entree_num=as.numeric(as.Date(coeur2$dateEntree)))
year(coeur2$dateSortie)<-1900+year(coeur2$dateSortie) %% 100
coeur2<-coeur2%>%mutate(date_sortie_num=as.numeric(as.Date(coeur2$dateSortie)))
```

Nous pouvons ensuite faire nos analyses avec le modèle de Cox. 
Pour une première analyse, nous estimerons notre modèle en prenant en compte toutes les covariables possible qui sont présentes dans la table de données. Mais le modèle de risques proportionnels de Cox fait plusieurs hypothèses. Ainsi, il est important d'évaluer si un tel modèle décrit correctement les données.

Ici, nous allons discuter de trois types de diagnostiques pour le modèle de Cox:

\begin{itemize}
\item Test de l'hypothèse des risques proportionnels.
\item Examination des observations influentes (ou les valeurs aberrantes).
\item Détection de la non-linéarité des variables.
\end{itemize}

## Détection de la non-linéarité des variables

Souvent, nous supposons que les covariables continues ont une forme linéaire. Cependant, cette hypothèse doit être vérifiée. Le traçage des résidus de Martingale par rapport à des covariables continues est une approche courante utilisée pour détecter la non-linéarité ou, en d'autres termes, pour évaluer la forme fonctionnelle d'une covariable. Pour une covariable continue donnée, les modèles du graphique peuvent suggérer que la variable n'est pas correctement ajustée.

La non-linéarité n'est pas un problème pour les variables catégorielles, nous n'examinons donc que les graphiques des résidus de martingale par rapport à une variable continue.
Testons avec le $\textbf{poids}$ : 

```{r,fig.width=5,fig.height=4,fig.align='center'}
survie=Surv(coeur2$date_entree_num,coeur2$date_sortie_num,coeur2$MCC)
res=coxph(survie~fibre+taille+poids+consommation+
            emploi+graisse,id=id,data=coeur2)
mresids <- residuals( res, type="martingale" )
lmfit <- lm(poids~taille+consommation+graisse+fibre,data=coeur2 )
rbili <- lmfit$resid
ord <- order( rbili )
mresids <- mresids[ ord ]
plot( rbili, mresids )
lines( smooth.spline( rbili, mresids, df=6 ), col="red", lwd=2 )
lines( rbili, fitted(lm( mresids ~ rbili )), col="blue", lwd=2 )

```

Il semble que le $\textbf{poids}$ soit linéaire.
Maintenant testons avec la variable $\textbf{fibre}$
```{r,fig.width=5,fig.height=4,fig.align='center',echo=FALSE}
mresids <- residuals( res, type="martingale" )
lmfit <- lm(fibre~poids+consommation+graisse+taille,data=coeur2 )

rbili <- lmfit$resid
ord <- order( rbili )
mresids <- mresids[ ord ]
plot( rbili, mresids )
lines( smooth.spline( rbili, mresids, df=6 ), col="red", lwd=2 )
lines( rbili, fitted(lm( mresids ~ rbili )), col="blue", lwd=2 )

```

Nous distinguons clairement 2 groupes de résidus et la courbe varie un peu. Nous allons appliquer une transformation logarithme à cette variable : 

```{r,fig.width=5,fig.height=4,fig.align='center',echo=FALSE}
mresids <- residuals( res, type="martingale" )
lmfit <- lm(log(fibre)~poids+consommation+graisse+log(taille),data=coeur2 )

rbili <- lmfit$resid
ord <- order( rbili )
mresids <- mresids[ ord ]
plot( rbili, mresids )
lines( smooth.spline( rbili, mresids, df=6 ), col="red", lwd=2 )
lines( rbili, fitted(lm( mresids ~ rbili )), col="blue", lwd=2 )

```

Nous voyons que c'est déja un peu mieux. Nous appliquerons donc le logarithme à $\textbf{fibre}$ dans nos modèles. Testons la consommation : \

```{r,fig.width=5,fig.height=4,fig.align='center',echo=FALSE}
mresids <- residuals( res, type="martingale" )
lmfit <- lm(consommation~poids+log(fibre)+graisse+taille,data=coeur2 )

rbili <- lmfit$resid
ord <- order( rbili )
mresids <- mresids[ ord ]
plot( rbili, mresids )
lines( smooth.spline( rbili, mresids, df=6 ), col="red", lwd=2 )
lines( rbili, fitted(lm( mresids ~ rbili )), col="blue", lwd=2 )

```

Il n'y a pas besoin d'appliquer de transformations, les résidus ne sont pas clairement entassés comme pour $\textbf{fibre}$.  
Testons pour $\textbf{graisse}$ :

```{r,fig.width=5,fig.height=4,fig.align='center',echo=FALSE}
mresids <- residuals( res, type="martingale" )
lmfit <- lm(graisse~consommation+poids+log(fibre)+log(taille),data=coeur2 )

rbili <- lmfit$resid
ord <- order( rbili )
mresids <- mresids[ ord ]
plot( rbili, mresids )
lines( smooth.spline( rbili, mresids, df=6 ), col="red", lwd=2 )
lines( rbili, fitted(lm( mresids ~ rbili )), col="blue", lwd=2 )

```

Comme précédemment, nous n'avons pas besoin d'appliquer de transformation.  
Testons désormais la $\textbf{taille}$ : 

```{r,fig.width=5,fig.height=4,fig.align='center',echo=FALSE}
mresids <- residuals( res, type="martingale" )
lmfit <- lm(taille~consommation+poids+log(fibre)+graisse,data=coeur2 )

rbili <- lmfit$resid
ord <- order( rbili )
mresids <- mresids[ ord ]
plot( rbili, mresids )
lines( smooth.spline( rbili, mresids, df=6 ), col="red", lwd=2 )
lines( rbili, fitted(lm( mresids ~ rbili )), col="blue", lwd=2 )

```

Essayons de voir ce qu'il se passe lorsque nous appliquons le logarithme : 

```{r,fig.width=5,fig.height=4,fig.align='center',echo=FALSE}
mresids <- residuals( res, type="martingale" )
lmfit <- lm(log(taille)~consommation+poids+log(fibre)+graisse,data=coeur2 )

rbili <- lmfit$resid
ord <- order( rbili )
mresids <- mresids[ ord ]
plot( rbili, mresids )
lines( smooth.spline( rbili, mresids, df=6 ), col="red", lwd=2 )
lines( rbili, fitted(lm( mresids ~ rbili )), col="blue", lwd=2 )

```

La courbe paraît plus stable. Nous ferons donc une transformation logarithme sur la variable $\textbf{taille}$.


## Examiner les observations influentes (ou les valeurs aberrantes).

Pour tester des observations influentes ou des valeurs aberrantes, nous pouvons utiliser les valeurs $dfbeta$. Le principe est de comparer le coefficient d'une variable donnée lorsque un point donné participe ou pas à la régression. Nous mesurons l’influence d’un point sur le coefficient estimé.

```{r}
ggcoxdiagnostics(res, type = "dfbeta",
                 linear.predictions = FALSE, ggtheme = theme_bw())
```

Nous voyons que la comparaison des valeurs les plus élevées aux coefficients de régression suggère qu'aucune des observations n'est influente individuellement.

## Test de l'hypothèse des risques proportionnels.

Regardons désormais si l’hypothèse d'indépendance du temps des formes multiplicatives et des covariables est vérifiée.
On peut tester l'hypothèse des risques proportionnels avec la fonction suivante : 

```{r}
res=coxph(survie~log(fibre)+log(taille)+poids+consommation+
            emploi+graisse,id=id,data=coeur2)
res.c=cox.zph(res)
res.c
```

Le test conduit à ne pas rejeter cette hypothèse au seuil de $5\%$ : aucune covariable n'a un effet dépendant du temps. 

## Modèle de Cox

Maintenant que nous avons vérifié ces trois hypothèses, nous allons analyser le modèle de Cox. 
Commençons par introduire toutes les variables dans le modèle.

```{r}
 res=coxph(survie~log(fibre)+log(taille)+poids+emploi+
             graisse,id=id,data=coeur2)
 summary(res)
```


Le test de Wald teste l’effet d’une covariable avec les autres covariable dans le modèle. S’il n’est pas significatif, cela ne veut pas dire qu’il ne le serait pas dans le modèle constitué uniquement de cette covariable.

Le test de Wald pour la covariable $\textbf{taille}$ montre que le coefficient correspondant est fortement significatif au seuil $5\%$, $p_{values} < 0.05$. Les autres covariables ne modifient pas significativement cette incidence lorsque $\textbf{taille}$ est dans le modèle.

Nous sélectionnons les variables pas à pas c'est à dire que nous enlèvons celle dont la $p_{value}$ est la plus élevée. Ensuite nous refaisons tourner le modèle et nous recommençons jusqu’à obtention de toutes les variables significatives.  

Nous obtenons le modèle suivant :

```{r}
res=coxph(survie~log(taille),id=id,data=coeur2)
summary(res)
confint(res)
```

En supposant que toutes les autres covariables sont fixées, l'augmentation de la taille de 1cm multiplie l'incidence instantanée de la maladie par au moins, $exp(-15.16573)=2.591834e-07$ et au plus $exp(-3.781619)=0.02278577$.


Auparavant, nous avont constater que dans le modèle constitué de toutes les covariables, tous les autres coefficients n'étaient pas significatifs au seuil de 5$\%$. Mais qu'en est-t-il si nous testons avec un modèle avec seulement 1 covariable. Regardons ce qui se passe pour $\textbf{fibre}$ :

```{r}
res=coxph(survie~log(fibre),id=id, data=coeur2)
summary(res)

confint(res)
```
\
La variable $\textbf{fibre}$  a un coefficient significatif au seuil 5$\%$ car $p_{values} =0.0293 < 0.05$. 

En supposant que toutes les autres covariables sont fixées, l’augmentation de fibre de 1 unité multiplie l'incidence instantanée de la maladie par au moins $exp(-1.531084)=0.2163011$.

Testons maintenant pour la variable $\textbf{graisse}$ 

```{r}
 res=coxph(survie~coeur2$graisse,id=coeur2$id)
summary(res)
confint(res)
```

La variable $\textbf{graisse}$ a une $p_{values}= 0.0668> 0.05$. De plus, l'intervalle de confiance n'est pas interpretable car il contient 0. Nous ne pouvons pas faire d'estimation avec ce modèle.


```{r}
res=coxph(survie~coeur2$emploi,id=coeur2$id)
summary(res)
confint(res)
```
Les intervalles de confiance comprennent 0, rien n'est interpretable.

\newpage

# Conclusion 

Nous avons appliqué dans ce projet toutes les méthodes étudiées lors de l'unité de "Biostatistiques" sur la table de données $\textbf{Coeur}$, ou du moins, celles qui étaient possible d'appliquer. Nous avons globalement compris les hypothèses du modèle de Cox qu'il fallait valider. En les appliquant, nous nous sommes rendu compte qu'il était assez compliqué de faire des interprétations précises sur la base de graphiques seulement et toutes les sources que nous avons trouvé utilisaient ces méthodes graphiques. 
Malheureusement nous étions à cours de temps pour approfondir encore plus nos recherches pour cette partie, mais l'essentiel a été compris. 

Au vu de nos résultat, nous pouvons dire que manger plus de fibre pourrait contribuer à la diminution de maladie.

\newpage

# Bibliographie 

Proportional Hazards Regression Diagnostics, Dan Gillen, 2016  

STHDA  $http://www.sthda.com/english/wiki/cox-model-assumptions$ 

Cours7- Exemple et programmation $http://iml.univ-mrs.fr/~reboul/duree62011.pdf$

Tests dans les modèles de durée $http://iml.univ-mrs.fr/~reboul/duree52013.pptx.pdf$

Le modèle de Cox $https://perso.univ-rennes1.fr/valerie.monbet/ExposesM2/2013/cox_model.pdf$


